# Hydra configuration for sampling benchmarks
# Usage: python run_benchmark.py [overrides]
# Example: python run_benchmark.py mcmc.steps=5 mcmc.alpha=2.0 benchmark.num_problems=20

defaults:
  - _self_

# Model configuration
model:
  name: grok-4-1-fast-non-reasoning
  base_url: https://api.x.ai/v1

# Benchmark configuration
benchmark:
  name: humaneval  # Options: humaneval, swebench, swebench-verified, gsm8k, mmlu
  num_problems: 10
  max_tokens: 128  # Max tokens for final completion (applied everywhere)

# Prompt customization
prompt:
  prefix: ""  # Text to add before the benchmark prompt
  suffix: "\n\nThink deeply and show your work."  # Text to add after the benchmark prompt

# MCMC sampling configuration (serial, single proposal per step)
mcmc:
  enabled: true
  alpha: 4.0                    # Power factor for target distribution pi(x) = p(x)^alpha
  steps: 5                      # Number of MCMC refinement steps
  top_logprobs: 5              # Number of top log probabilities to retrieve
  proposal_temperature: 0.25    # Temperature for proposal distribution, should be alpha^(-1)
  restrict_to_last_n: null      # Only resample last N blocks (null = disabled, int = enabled)
  block_size: 32                # Block size B for block-wise generation (paper default)
  debug: true                   # Print debug info during MCMC (acceptance/rejection, log_r, etc.)

# Parallel MCMC sampling configuration (multiple proposals per step)
# Based on: "A general construction for parallelizing Metropolis-Hastings algorithms" (Calderhead 2014, PNAS)
mcmc_parallel:
  enabled: false
  alpha: 1.67                    # Power factor for target distribution pi(x) = p(x)^alpha
  steps: 5                       # Number of MCMC refinement steps per block
  top_logprobs: 5                # Number of top log probabilities to retrieve
  proposal_temperature: 0.59     # Temperature for proposal distribution
  block_size: 192                # Block size B for block-wise generation
  debug: true                    # Print debug info
  # Parallel-specific parameters:
  num_proposals: 4               # N proposals per MCMC step (includes current state)
  max_concurrent: 10             # Max concurrent API requests
  timeout: 60.0                  # Per-request timeout (seconds)
  max_retries: 3                 # Retry count for failed requests

# Temperature sampling configuration
temperature_sampling:
  enabled: false
  temperature: 0.8

# Greedy sampling configuration
greedy:
  enabled: true

# Output configuration
output:
  verbose: true
  save_results: false
  results_dir: ./results
